{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction, part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"https://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons Licence\" style=\"width=50\" src=\"https://licensebuttons.net/l/by-nc-sa/4.0/88x31.png\" title='This work is licensed under a Creative Commons Attribution 4.0 International License.' align=\"right\"/></a>\n",
    "\n",
    "**Authors**: Dr Matteo Degiacomi (matteo.t.degiacomi@durham.ac.uk) and Dr Antonia Mey (antonia.mey@ed.ac.uk)\n",
    "\n",
    "Content is partially adapted from the [Software Carpentries Machine learning lesson](https://carpentries-incubator.github.io/machine-learning-novice-sklearn/index.html) and material from the [pyEMMA tutorial](http://www.emma-project.org/latest/tutorials/notebooks/02-dimension-reduction-and-discretization.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "How can we perform unsupervised learning with dimensionality reduction techniques such as Principal Component Analysis (PCA), time-correlated independent component analysis (tICA), and t-distributed Stochastic Neighbor Embedding (t-SNE)?\n",
    "\n",
    "**Objectives:**\n",
    "- Remember that most data is inherently multidimensional\n",
    "- Understand that reducing the number of dimensions can simplify modelling and allow classifications to be performed.\n",
    "- Use PCA as a popular technique for dimensionality reduction.\n",
    "- Use tICA another popular dimensionality reduction technique that takes time correlations into account\n",
    "- t-SNE is another technique for dimensionality reduction.\n",
    "- Apply PCA and t-SNE with Scikit Learn to an example dataset.\n",
    "- Compare how PCA and tICA perform on a 2-D toy example\n",
    "- Evaluate the relative performance of PCA and t-SNE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jupyter cheat sheet**:\n",
    "- to run the currently highlighted cell, hold <kbd>&#x21E7; Shift</kbd> and press <kbd>&#x23ce; Enter</kbd>;\n",
    "- to get help for a specific function, place the cursor within the function's brackets, hold <kbd>&#x21E7; Shift</kbd>, and press <kbd>&#x21E5; Tab</kbd>;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "nbgrader": {},
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Scientific data, such as that extracted from molecular dynamics simulations, can be high-dimensional and noisy. Dimensionality reduction is the process identifying and highlighting information and correlations within the data. There are multiple reasons why you might want to do a dimensionality reduction.\n",
    "\n",
    "- You might want to know what are the dominant features in your system (larger scale variations in data).\n",
    "- You want a way to visualise your high dimensional data. \n",
    "- You want to analyse your data, but it it too high-dimensional.\n",
    "\n",
    "The algorithms designed to carry out this task are an example of machine learning. In this tutorial we will look at Principal Components Analysis (PCA), time-lagged independent component analysis (tICA), and t-tested Stocastic Neighbour Embedding (t-SNE). In a machine learning context, each dimension in data is called a **feature**, which together form a **feature space**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 1:</b> Can you think of examples of features that you would find in molecular simulations?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution: </mark> </summary>\n",
    "\n",
    "Examples are:\n",
    "- C-alpha positions\n",
    "- angles\n",
    "- dihedrals\n",
    "- RMSD\n",
    "- density\n",
    "- surface area\n",
    "- ...\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Principal Components Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) is an orthogonal linear transformation that transforms high dimensional data (or feature vectors) into a new coordinate system. In this coordinate system the first coordinate (first *eigenvector*) corresponds to the scalar projection of a linear combination of some data such that this coordinate has the largest variance. The second largest variance in the data can be found in the second coordinate and so on. Let's start by importing some packages required for the our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create some model data to analyse. To this end, we will exploit the Müller-Brown potential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def muller_potential(x, y):\n",
    "    \"\"\"Muller potential\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : {float, np.ndarray}\n",
    "        X coordinate. Can be either a single number or an array. If you supply\n",
    "        an array, x and y need to be the same shape.\n",
    "    y : {float, np.ndarray}\n",
    "        Y coordinate. Can be either a single number or an array. If you supply\n",
    "        an array, x and y need to be the same shape.\n",
    "    Returns\n",
    "    -------\n",
    "    potential : {float, np.ndarray}\n",
    "        Potential energy. Will be the same shape as the inputs, x and y.\n",
    "    \n",
    "    Reference\n",
    "    ---------\n",
    "    Code adapted from https://cims.nyu.edu/~eve2/ztsMueller.m\n",
    "    \"\"\"\n",
    "    \n",
    "    aa = [-1, -1, -6.5, 0.7]\n",
    "    bb = [0, 0, 11, 0.6]\n",
    "    cc = [-10, -10, -6.5, 0.7]\n",
    "    AA = [-200, -100, -170, 15]\n",
    "    XX = [1, 0, -0.5, -1]\n",
    "    YY = [0, 0.5, 1.5, 1]\n",
    "    \n",
    "    value = 0\n",
    "    for j in range(0, 4):\n",
    "        value += AA[j] * np.exp(aa[j] * (x - XX[j])**2 + \\\n",
    "            bb[j] * (x - XX[j]) * (y - YY[j]) + cc[j] * (y - YY[j])**2)\n",
    "    return value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now evaluate the potential on a 2-D grid, and plot visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = (500, 500)\n",
    "x = np.linspace(-1.5, 1, dims[0])\n",
    "y = np.linspace(-0.4, 1.8, dims[1])\n",
    "X, Y = np.meshgrid(x, y)\n",
    "potential = muller_potential(X, Y)\n",
    "\n",
    "levels = np.linspace(np.min(potential), np.max(potential), 50)\n",
    "plt.contour(X, Y, potential.clip(max=200), 40);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's convert our potential into a probability distribution, and plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.sum(np.exp(-1/25*potential)) #partition function\n",
    "P = np.exp(-1/25*potential)/Z\n",
    "\n",
    "plt.contour(X, Y, P, 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to generate some data! We will extract 10000 samples according to the probability distribution we have just created. To this end, we will use np.random.choice, which enables us to generate random samples according to a given probability. Since this method works only in 1-D, we will first flatten the array, generate the samples, and them bring them back to 2-D.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat = np.ravel(P)\n",
    "sample_index = np.random.choice(a=flat.size, p=flat, size=10000)\n",
    "samples = np.unravel_index(sample_index, P.shape)\n",
    "data = np.array([x[samples[1]], y[samples[0]]]).T\n",
    "\n",
    "plt.scatter(data[:, 0], data[:, 1], c=\"r\", alpha=0.05);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA can tell us in which features (here the x coordinate is one feature and the y coordinate is the second feature), or rather which linear combination, carries the most variance. Let's do a PCA of the samples we have created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PCA has identified the two eigenvectors (principal components) of our dataset. Here they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each component represents a percentage of the total variance of the system. Here is how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It is clear that the first component represents majority of the variance in the data. We have identified the two eigenvectors of this dataset. We can now plot them along with the data. To make arrows visible, we will scale their length by the explained variance of each eigenvector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter(data[:,0], data[:,1], c=\"r\", alpha=0.05)\n",
    "\n",
    "# plot arrows representing the two components\n",
    "e1_x = pca.components_[0, 0]*pca.explained_variance_ratio_[0]\n",
    "e1_y = pca.components_[0, 1]*pca.explained_variance_ratio_[0]\n",
    "e2_x = pca.components_[1, 0]*pca.explained_variance_ratio_[1]\n",
    "e2_y = pca.components_[1, 1]*pca.explained_variance_ratio_[1]\n",
    "ax.arrow(np.mean(x), np.mean(y), e1_x/pca.explained_variance_ratio_[0], e1_y/pca.explained_variance_ratio_[0], head_width=0.1, head_length=0.1, fc='k', ec='k')\n",
    "ax.arrow(np.mean(x), np.mean(y), e2_x/pca.explained_variance_ratio_[0], e2_y/pca.explained_variance_ratio_[0], head_width=0.1, head_length=0.1, fc='k', ec='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can project the data on the new reference system defined by the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_projected = data.dot(pca.components_) + pca.mean_\n",
    "print(data_projected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 2: </b> Can you use the PCA results to generate an 1-D approximate for the Müller-Brown potential?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution</mark> </summary>\n",
    "\n",
    "The first principal component represents most of the variance, so we can observe the distribution of data only along this components. This is a way of using PCA as a way of filtering noise, and highlight dominant structures in your data.\n",
    "    \n",
    "```Python\n",
    "plt.hist(data_projected[:,0], bins=50, color=\"r\");\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The MNIST Dataset\n",
    "\n",
    "The MNIST dataset consists of a 60,000 examples of hand written numbers and 10,000 test set examples. The digits have all been resized to the same size and centered within this fixed image size. One way of accessing the data is from [here](http://yann.lecun.com/exdb/mnist/) or we can use built in function with scikit-learn. In this way you will access a reduced part of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import datasets\n",
    "from sklearn import manifold\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# Examine the dataset\n",
    "print(digits.data)\n",
    "print(digits.target)\n",
    "\n",
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A short helper function to plot an example from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_digits(X):\n",
    "    \"\"\"Small helper function to plot 100 digits.\"\"\"\n",
    "    fig, axs = plt.subplots(nrows=10, ncols=10, figsize=(8, 8))\n",
    "    for img, ax in zip(X, axs.ravel()):\n",
    "        ax.imshow(img.reshape((8, 8)), cmap=\"Greys\")\n",
    "        ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digits(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 3:</b> Understanding the dataset\n",
    "\n",
    "- What are the dimensions of the data?\n",
    "- What are the features of the data?\n",
    "- What information do the features hold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your solution here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution: </mark> </summary>\n",
    "\n",
    "```Python\n",
    "    \n",
    "# data dimension\n",
    "np.shape(X)\n",
    "# The output of the array tells you, that you have 1797 samples of a 64 dimensional feature vector\n",
    "  \n",
    "# Features\n",
    "print(X[0])\n",
    "# Each entry in the first sample of the feature vector gives you a value of the grey scale from the image. This could be normalised.\n",
    "```\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 4:</b> Can you do a principal component analysis of the digits dataset?\n",
    "\n",
    "- Do a PCA analysis using two components.    \n",
    "     - What is the variance contribution of the first two components?    \n",
    "     - How many components do you need to reach 90% variance explained?    \n",
    "- Generate a plot of the first two principal components and colour them according to your digit. What does it tell you? </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your solution here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution: </mark> </summary>\n",
    "\n",
    "```Python\n",
    "    \n",
    "# PCA\n",
    "pca = decomposition.PCA(n_components=2, n_oversamples=20)\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "\n",
    "# variance contribution of first two components \n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "# finding 90% variance\n",
    "for i in range(2,30):\n",
    "    pca = decomposition.PCA(n_components=i, n_oversamples=20)\n",
    "    pca.fit(X)\n",
    "    X_pca = pca.transform(X)\n",
    "    cum_sum = pca.explained_variance_ratio_.cumsum()\n",
    "    if cum_sum[-1] > 0.9:\n",
    "        print(f'{i} number of principle components are needed to reach a variance explained of 90%')\n",
    "        break\n",
    "\n",
    "# plotting first two components and colouring according to digits\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=plt.cm.nipy_spectral, \n",
    "        edgecolor='k',label=y)\n",
    "plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\n",
    "plt.xlabel('pc 1')\n",
    "plt.ylabel('pc 2') \n",
    "```\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 5: </b> Rerun your PCA with 5 components </div>\n",
    "\n",
    "What feature in your 64 (8x8) digit input vector contributes the most to your first principle components? \n",
    "Hint: look at the absolute value of `pca.components_[0]` If you you generate a bar plot you can see the contributions well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your solution here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution: </mark> </summary>\n",
    "\n",
    "```Python\n",
    "pca = decomposition.PCA(n_components=5, n_oversamples=20)\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "indeces = np.argsort(abs(pca.components_[0]))\n",
    "x = np.linspace(0,len(indeces), len(indeces))\n",
    "ax.bar( x,abs(pca.components_[0])[indeces],tick_label=indeces)\n",
    "ax.set_xlabel('feature index')\n",
    "ax.set_ylabel('absolute value of contribution')\n",
    "```\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. time-lagged independent component analysis (tICA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will study tCA with a toy dataset. Let's start by loading it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "file = 'data/hmm-doublewell-2d-100k.npz'\n",
    "with np.load(file) as fh:\n",
    "    data = fh['trajectory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Visualising the dataset\n",
    "We can see this is a trajectory with 100000 time datapoints and 2 features. Let's examine this dataset a bit more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "axes[0].plot(data[:300,0], alpha=0.6)\n",
    "axes[1].plot(data[:300,1], alpha=0.6)\n",
    "axes[1].set_xlabel('$time$')\n",
    "axes[0].set_xlabel('$time$')\n",
    "axes[0].set_ylabel('$x$')\n",
    "axes[1].set_ylabel('$y$')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 6: </b> Examine the data a bit more, to get a better feel for it. What is the extent of the data set in x and y? Can you plot a histogram of the data? What information does the trajectory tell us the histogram obscures? </div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution: </mark> </summary>\n",
    "\n",
    "The minimum and maximum of the data is given by:\n",
    "\n",
    "```Python\n",
    "print('x_min is:',np.min(data[:,0]), 'x_max is:',np.max(data[:,0]), '\\ny_min is:', np.min(data[:,1]), 'y_max is:', np.max(data[:,1]))\n",
    "\n",
    "```\n",
    "\n",
    "An example of how to plot a histogram of the data looks like this:\n",
    "    \n",
    "```Python\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "counts,ybins,xbins = np.histogram2d(data[:,0],data[:,1],bins=250);\n",
    "plt.contour(counts,extent=[xbins.min(),xbins.max(),ybins.min(),ybins.max()])\n",
    "```\n",
    "There is no slow transitions in the x coordinate, but there are in the y coordinate.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. tICA analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tICA is a common dimensionality reduction technique for molecular dynamics trajectories. Unfortunately scikit-learn does not feature an implementation of this method, which is why other packages are normally used. Here, we provide a convenient helper module based on the implementation from [MSM-Builder](http://msmbuilder.org/3.8.0/), adapted so that it can be used as stand-alone code. The module is written so as to mimic the dimensionality reduction is done with scikit-learn. That means you use a similar syntax as before, i.e you create an instance of tICA with a given number of components and then you use fit and transform on the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tica.tica import tICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's carry out a tICA analysis of the data we have previously loaded. A small difference from the syntax in scikit-learn: the parameter of the <code>fit</code> method data must be in square brackets, since the method can accept a list of trajectory data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tic = tICA()\n",
    "tic.fit([data]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The dimensions identified by the tICA analysis can be accessed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tic.eigenvectors_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time for the next excercise! Before getting to it though, execute the cell below, you will need it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_arrow(origin, v, color):\n",
    "    ax.arrow(origin[0], origin[1], v[0], v[1], color=color, width=0.02, linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 7: </b> Now use PCA on the same dataset and compare the two components by creating a scatter plot and drawing the vectors representing the PCA composition and tICA composition. Make use of the handy helper function <code>draw_arrow</code> for the vectors to draw arrows on the scatter plot from above. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your solution here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution: </mark> </summary>\n",
    "\n",
    "We start by carrying out the PCA of the toy data\n",
    "```Python\n",
    "pca = PCA()\n",
    "pca.fit(data);\n",
    "```\n",
    "\n",
    "Now, we plot a scatterplot of data, with arrows representing the first components of both tICA (red) and PCA (blue).\n",
    "\n",
    "```Python\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "ax.scatter(data[:,0], data[:,1], marker = '.', color='black', alpha=0.1)\n",
    "\n",
    "origin = np.mean(data, axis=0)\n",
    "draw_arrow(origin, tic.eigenvectors_[0]*2, \"red\")\n",
    "draw_arrow(origin, pca.components_[0]*2, \"dodgerblue\")\n",
    "\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\");\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Comparison of dimensionality reduction with PCA and tICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction techniques enable us to identify suitable ways of projecting high-dimensional data into a lower-dimensional space with minimal information loss. We have just seen that PCA and tICA identify different spaces onto which the data can be projected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 5: </b>Project the data into the eigenspace generated by PCA, and into the tICA space. Create histograms of each of the components. What do you observe? </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your solution here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution: </mark> </summary>\n",
    "\n",
    "Let's project the data into the tICA and PCA spaces.\n",
    "    \n",
    "```Python    \n",
    "tic_out = tic.transform([data])[0]\n",
    "PCA_out = pca.transform(data)\n",
    "```\n",
    "    \n",
    "Now, let's make some pretty plots showing the projections on the first and second component of PCA and tICA\n",
    "    \n",
    "```Python\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "    \n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.hist(tic_out[:, 0], histtype=\"step\", label=\"tICA\", bins=50, color=\"red\")\n",
    "ax1.hist(PCA_out[:, 0], histtype=\"step\", label=\"PCA\", bins=50, color=\"dodgerblue\")\n",
    "ax1.set_xlabel(\"first component\")\n",
    "ax1.set_ylabel(\"count (#)\")\n",
    "ax1.legend(frameon=False);\n",
    "\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.hist(tic_out[:, 1], histtype=\"step\", label=\"tICA\", bins=50, color=\"red\")\n",
    "ax2.hist(PCA_out[:, 1], histtype=\"step\", label=\"PCA\", bins=50, color=\"dodgerblue\")\n",
    "ax2.set_xlabel(\"second component\")\n",
    "ax2.legend(frameon=False);\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. t-Distributed Stochastic Neighbor Embedding (t-SNE) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE is a technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets. It gives each datapoint a poisition in a two or three dimensional map. It is classed as a non-linear dimensionality reduction technique and models high-dimensional data that are close in space to spatially close two or three-deminsional points. Let's apply t-SNE to the MNIST dataset we have met in section 3!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = manifold.TSNE(n_components=2, init='pca', random_state = 0)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "fig = plt.figure(1, figsize=(4, 4))\n",
    "plt.clf()\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap=plt.cm.nipy_spectral,\n",
    "        edgecolor='k',label=y)\n",
    "plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\n",
    "plt.xlabel('tsne 0')\n",
    "plt.ylabel('tsne 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 6:</b> Can you regenerate your t-SNE embedding in 3D and plot it? </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution:</mark> </summary>\n",
    "\n",
    "```Python\n",
    "tsne = manifold.TSNE(n_components=3, init='pca', random_state = 0)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "fig = plt.figure(1, figsize=(4, 4))\n",
    "plt.clf()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(X_tsne[:, 0], X_tsne[:, 1], X_tsne[:, 2], c=y, cmap=plt.cm.nipy_spectral, s=9, lw=0)\n",
    "\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Key points:</b>\n",
    "\n",
    "- PCA is a linear dimensionality reduction technique for tabular data,\n",
    "- PCA can be used to remove noise from data,\n",
    "- tICA is also a linear dimensionality reduction technique, but it maximises the autocorrelation time rather than the variance    \n",
    "- tICA and PCA may be appropriate for different use cases: tICA will generally provide you with slow dynamics and PCA for maximising spacial variance. \n",
    "- t-SNE is another dimensionality reduction technique for tabular data that is more general than PCA.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Dimensionality Reduction, part 2](2_DL_part2.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
